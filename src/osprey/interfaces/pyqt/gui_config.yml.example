# ============================================================================
# Osprey GUI Configuration Template
# ============================================================================
#
# This is an example configuration file for the Osprey PyQt GUI.
# Copy this file to 'gui_config.yml' in the same directory and update
# the values below for your facility's setup.
#
# REQUIRED STEPS:
# 1. Copy this file: cp gui_config.yml.example gui_config.yml
# 2. Configure your LLM provider settings (models and api sections)
# 3. Set environment variables for API keys (see env.example in project root)
# 4. Adjust GUI settings as needed for your facility
#
# ============================================================================

# ----------------------------------------------------------------------------
# Project Configuration
# ----------------------------------------------------------------------------
# Basic project settings
#
project_root: .

# Build directory for container deployments (not used by GUI directly)
# The GUI uses _agent_data for runtime data (conversations, checkpoints, etc.)
build_dir: ./build

# ----------------------------------------------------------------------------
# LLM Model Configuration (REQUIRED)
# ----------------------------------------------------------------------------
# Configure which models to use for different GUI operations.
# The 'classifier' model is REQUIRED for multi-project routing and orchestration.
#
# Recommended: Claude Haiku 3.5 for best performance
# Supported providers: anthropic, openai, argo, cborg, stanford, google, ollama
#
# IMPORTANT: Replace 'your_provider' and 'your_model' below with your actual
# provider and model configuration.
#
models:
  # REQUIRED: Model used for routing queries between projects
  classifier:
    provider: your_provider  # e.g., anthropic, openai, argo, cborg, stanford, google, ollama
    model_id: your_model     # e.g., claude-3-5-haiku-20241022, gpt-4o-mini
    max_tokens: 4096

  # OPTIONAL: Model used for orchestrating multi-project queries
  # If not specified, will use the classifier model
  orchestrator:
    provider: your_provider
    model_id: your_model
    max_tokens: 4096
  
  # Framework infrastructure models (optional - will use classifier if not specified)
  task_extraction:
    provider: your_provider
    model_id: your_model
  response:
    provider: your_provider
    model_id: your_model
  approval:
    provider: your_provider
    model_id: your_model
  memory:
    provider: your_provider
    model_id: your_model
  python_code_generator:
    provider: your_provider
    model_id: your_model
    max_tokens: 8192
  time_parsing:
    provider: your_provider
    model_id: your_model


# ----------------------------------------------------------------------------
# API Provider Configuration (REQUIRED)
# ----------------------------------------------------------------------------
# Configure API credentials and endpoints for your LLM provider(s).
# Use environment variables (${VAR_NAME}) for API keys - never hardcode them!
#
# Set these environment variables in your shell or .env file:
# - ANTHROPIC_API_KEY
# - OPENAI_API_KEY
# - CBORG_API_KEY
# - STANFORD_API_KEY
# - GOOGLE_API_KEY
# - ARGO_API_KEY (if using Argonne's service)
#
api:
  providers:
    # Anthropic (Claude models) - Recommended
    anthropic:
      api_key: ${ANTHROPIC_API_KEY}
      base_url: https://api.anthropic.com

    # OpenAI (GPT models)
    openai:
      api_key: ${OPENAI_API_KEY}
      base_url: https://api.openai.com/v1

    # LBNL CBORG service
    cborg:
      api_key: ${CBORG_API_KEY}
      base_url: https://api.cborg.lbl.gov/v1

    # Stanford AI Playground
    stanford:
      api_key: ${STANFORD_API_KEY}
      base_url: https://api.stanford.edu/v1

    # Google Gemini
    google:
      api_key: ${GOOGLE_API_KEY}
      base_url: https://generativelanguage.googleapis.com/v1beta

    # Argonne National Laboratory service
    argo:
      api_key: ${ARGO_API_KEY}
      base_url: https://argo-bridge.cels.anl.gov/v1

    # Ollama (local models)
    ollama:
      api_key: ollama  # Not used for Ollama
      base_url: http://localhost:11434
      host: localhost
      port: 11434


# ----------------------------------------------------------------------------
# GUI Settings
# ----------------------------------------------------------------------------
# Configure GUI behavior and appearance
#
gui:
  # Conversation persistence
  use_persistent_conversations: true
  
  # Storage mode for conversation history
  # Options: 'json' (file-based) or 'postgresql' (database)
  conversation_storage_mode: json
  
  # PostgreSQL connection (only needed if conversation_storage_mode is 'postgresql')
  # postgresql_uri: postgresql://user:password@localhost:5432/osprey_gui
  
  # Output redirection
  redirect_output_to_gui: true
  suppress_terminal_output: false
  
  # Message display
  group_system_messages: true
  
  # Routing feedback
  enable_routing_feedback: true


# ----------------------------------------------------------------------------
# Routing Configuration
# ----------------------------------------------------------------------------
# Advanced settings for multi-project routing and orchestration
#
routing:
  # Query caching for faster repeated queries
  cache:
    enabled: true
    max_size: 100
    ttl_seconds: 3600.0  # 1 hour
    similarity_threshold: 0.85
  
  # Advanced cache invalidation strategies
  advanced_invalidation:
    enabled: true
    adaptive_ttl: true
    probabilistic_expiration: true
    event_driven: true
  
  # Semantic analysis for better routing decisions
  semantic_analysis:
    enabled: true
    similarity_threshold: 0.5
    topic_similarity_threshold: 0.6
    max_context_history: 20
  
  # Multi-project orchestration
  orchestration:
    max_parallel: 3  # Maximum parallel sub-queries
  
  # Analytics and metrics
  analytics:
    max_history: 1000  # Maximum routing decisions to track
  
  # User feedback collection
  feedback:
    enabled: true


# ----------------------------------------------------------------------------
# Agent Control Settings
# ----------------------------------------------------------------------------
# Control agent behavior and capabilities
#
execution_control:
  # Agent control
  agent_control:
    task_extraction_bypass_enabled: false
    capability_selection_bypass_enabled: false
  
  # EPICS control system settings (if applicable)
  epics:
    writes_enabled: false  # Set to true to allow EPICS writes
  
  # Execution limits
  limits:
    max_reclassifications: 1
    max_planning_attempts: 2
    max_step_retries: 0
    max_execution_time_seconds: 300
    max_concurrent_classifications: 5


# ----------------------------------------------------------------------------
# Approval Settings
# ----------------------------------------------------------------------------
# Configure human approval requirements for sensitive operations
#
approval:
  # Global approval mode
  # Options: 'all' (approve everything), 'selective' (approve specific capabilities), 'none' (no approval)
  global_mode: selective
  
  # Capability-specific approval settings
  capabilities:
    # Python code execution approval
    python_execution:
      enabled: true
      mode: all_code  # Options: 'all_code', 'epics_writes', 'none'
    
    # Memory storage approval
    memory:
      enabled: true


# ----------------------------------------------------------------------------
# Development Settings
# ----------------------------------------------------------------------------
# Settings for debugging and development
#
development:
  # Debug mode (enables DEBUG logging level)
  debug: false
  
  # Verbose logging
  verbose_logging: false
  
  # Error handling
  raise_raw_errors: false
  
  # Prompt visibility
  prompts:
    print_all: false
    show_all: false
    latest_only: true
  
  # Memory monitoring
  memory_monitor:
    enabled: true
    warning_threshold_mb: 500
    critical_threshold_mb: 1000
    check_interval_seconds: 5


# ----------------------------------------------------------------------------
# Memory Monitoring (Top-level)
# ----------------------------------------------------------------------------
# System memory monitoring thresholds
#
memory_monitoring:
  enabled: true
  warning_threshold_mb: 500
  critical_threshold_mb: 2000
  check_interval_seconds: 5


# ----------------------------------------------------------------------------
# File Paths
# ----------------------------------------------------------------------------
# Data storage organization
#
file_paths:
  agent_data_dir: _agent_data
  user_memory_dir: user_memory
  execution_plans_dir: execution_plans
  conversations: conversations
  registry_exports_dir: registry_exports